# HotHash: Hotness-Aware Consistent Hashing for Cloud Databases

## Setup the experiments

**Google Cloud Platform**

1. The related files are located under `./gcp`.
2. This project requires a cloud environment consist of a coordinator node, a set of worker nodes, and a storage bucket. Related configurations (i.e. storage bucket name, node IPs) should be put to the `config.json` file.
3. All nodes should be initialized using `init-node.bash`.
4. Each worker node should have a copy of `server.py` running.
5. The coordinator node should have the `exp.py` file.

**Simulation**

1. The related files are located under `./sim`.
2. The dependencies could be installed by running `init.bash`.

## Generate data

**Google Cloud Platform**

Data could be generated by running the `generate_data.py`. The size and number of data blocks could be set by adjusting the parameters.

**Simulation**

The size and number of data blocks could be set by adjusting parameters in the controlling scripts.

## Run the experiments

**Google Cloud Platform**

1. In order to run cloud experiments, first start all worker nodes with `server.py` running.
2. Run the controlling script `batch-exp.py` and `batch-exp-new.py`. Experiment parameters could be configured by changing the `*_ARGS` in the controlling scripts.

**Simulation**

1. Run simulations for different methods using the following controlling scripts:
    - Consistent Hashing: `cons.py`
    - Bounded Load: `bound.py`
    - Random Jump: `rjump.py`
    - SPORE: `spore.py`
    - HotHash: `hot.py`

2. The data size, number and balancing parameters could be adjust in the above controlling scripts.
3. The query workload could be adjust in `dataset.py`.
4. Hardware-related configurations (i.e. CPU capacity, network bandwidth) could be adjust in `controller.py` and `entities.py`.

## Theoretical Analysis
We include the full theoretical analysis and proof as appendix in the `HotHash Technical Report.pdf`.
